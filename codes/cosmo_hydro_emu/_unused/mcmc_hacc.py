# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_mcmc.ipynb.

# %% auto 0
__all__ = ['ln_prior', 'ln_like', 'ln_prob', 'chain_init', 'define_sampler', 
'do_mcmc', 'mcmc_results', 'log_likelihood']

# %% ../nbs/04_mcmc.ipynb 3

import numpy as np
import emcee 
import time
from cosmo_hydro_emu.emu import emu_redshift, emulate
from cosmo_hydro_emu.load_hacc import PARAM_NAME, delta_cgd


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_label=None):
    if fixed_params is None:
        fixed_params = {}

    param_names = PARAM_NAME
    full_params = []

    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1
    
    if with_underestimation_bias:
        log_f = theta[-1]  # Assume last element is log_f when with_underestimation_bias is True

    
    full_params = np.array(full_params)
    # print(full_params)
    
    model_grid, model_var_grid = emulate(sepia_model, full_params)
    
    if case_label=='CGD_nodelta': # keep to 'CGD' for yesdelta option, keep 'CGD_nodelta' for nodelta option
        model_grid = model_grid + delta_cgd[:, np.newaxis]
        model_var_grid = model_var_grid + delta_cgd[:, np.newaxis, np.newaxis]

    model = np.interp(x, x_grid, model_grid[:, 0])
    model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
    
    if with_underestimation_bias:
        sigma2 = yerr**2 + model**2 * np.exp(2 * log_f)
        ll = -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))
    else:
        sigma2 = yerr**2
        ll = -0.5 * np.sum(( y - model) ** 2 / sigma2)
    return ll


# def ln_like(theta, x_grids, sepia_models, data, fixed_params=None, with_underestimation_bias=False):
#     log_likelihoods = []
    
#     for i in range(len(sepia_models)):
#         ll = log_likelihood(theta, 
#                             x_grids[i], 
#                             sepia_models[i], 
#                             data[i]['x'], 
#                             data[i]['y'], 
#                             data[i]['yerr'], 
#                             fixed_params=fixed_params, 
#                             with_underestimation_bias=with_underestimation_bias)
#         log_likelihoods.append(ll)

#     total_ll = sum(log_likelihoods)
    
#     return total_ll

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None,
            with_underestimation_bias=False,
            case_labels=None):
    log_likelihoods = []
    case_labels = case_labels.split('_')
    normalize_log_like = False
    
    for i in range(len(sepia_models)):
        ll = log_likelihood(theta, 
                            x_grids[i], 
                            sepia_models[i], 
                            data[i]['x'], 
                            data[i]['y'], 
                            data[i]['yerr'], 
                            fixed_params=fixed_params, 
                            with_underestimation_bias=with_underestimation_bias,
                            case_label=case_labels[i])

        if normalize_log_like: 
            variance = np.var(data[i]['y'])
            log_likelihoods.append(ll / variance)
        else:
            log_likelihoods.append(ll)

    total_ll = sum(log_likelihoods)
    
    return total_ll


def ln_prob(theta, 
            params_list, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):

    lp = ln_prior(theta, params_list)
    if not np.isfinite(lp):
        return -np.inf
    return lp + ln_like(theta, x_grids, sepia_models, data, fixed_params=fixed_params, with_underestimation_bias=with_underestimation_bias, case_labels=case_labels)




## Default prior 

def ln_prior(theta, params_list):
    pdf_sum = 0
    for p, param in zip(theta, params_list):
        if not (param[2] < p < param[3]):
            return -np.inf
        p_mu = 0.5 * (param[3] - param[2]) + param[2]
        p_sigma = 1 * (param[3] - p_mu)
        pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    return pdf_sum

# def ln_prior(theta, params_list):
#     for p, param in zip(theta, params_list):
#         # Check if parameter is within bounds
#         if not (param[2] < p < param[3]):
#             return -np.inf
    
#     # If all parameters are within bounds, return 0.0 (log of 1.0)
#     return 0.0

def ln_prior(theta, params_list):
    
    """
    Mixed prior function that allows specification of which parameters should have flat priors.
    
    Parameters:
    -----------
    theta : array-like
        Parameter values being evaluated
    params_list : list
        List of parameter specifications [name, initial_value, lower_bound, upper_bound]
    flat_indices : list or None
        Indices of parameters that should have flat priors. If None, all priors are Gaussian.
    
    Returns:
    --------
    float
        Log probability of the prior
    """

    flat_indices=[4] # 4 == eps_kin is flat, rest are gaussian

    if flat_indices is None:
        flat_indices = []
    
    pdf_sum = 0
    
    for i, (p, param) in enumerate(zip(theta, params_list)):
        # Check if parameter is within bounds
        if not (param[2] < p < param[3]):
            return -np.inf
        
        # Apply flat prior for specified indices, Gaussian prior for others
        if i in flat_indices:
            # Flat prior - contributes nothing to the pdf_sum
            continue
        else:
            # Gaussian prior - same as your original function
            p_mu = 0.5 * (param[3] - param[2]) + param[2]
            p_sigma = 1 * (param[3] - p_mu)
            pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    
    return pdf_sum
# ########################################################################################################################


### Strong prior for EPS > 5
# from scipy.stats import halfnorm

# def ln_prior(theta, params_list):
#     pdf_sum = 0
#     p_loc = 0.55
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         if not (param[2] < p < param[3]):
#             return -np.inf
        
#         if i == len(theta) - 1:  # Last parameter
#             if p < p_loc:
#                 return -np.inf
#             p_sigma = 0.5 * (param[3] - p_loc)
#             pdf = halfnorm(loc=p_loc, scale=p_sigma).pdf(p)
#         else:
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)
#             pdf = (1.0 / (np.sqrt(2 * np.pi) * p_sigma)) * np.exp(-0.5 * (p - p_mu) ** 2 / p_sigma ** 2)
        
#         pdf_sum += np.log(pdf)
    
#     return pdf_sum


### Very Strong prior for 3 parameters

########################################################################################################################

# def ln_prior(theta, params_list):

#     prior_info_mask = [True, True, True, False, False]
#     prior_locs = [3.0, 0.5, 0.8]

#     pdf_sum = 0
#     prior_index = 0
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         if not (param[2] < p < param[3]):
#             return -np.inf

#         if prior_info_mask[i]:
#             p_mu = prior_locs[prior_index]
#             p_sigma = 0.05 * (param[3] - param[2])  # Tight prior
#             prior_index += 1
#         else:
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)  # Uninformed prior
        
#         pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
        
#     return pdf_sum


############################################################
########################################################################################################################

# def chain_init(params_list, ndim, nwalkers):
#     pos0 = [[param[1] * 1.0 for param in params_list] + 1e-3 * np.random.randn(ndim) for _ in range(nwalkers)]
#     return pos0


# def chain_init(params_list, ndim, nwalkers):
#     pos0 = []
#     for _ in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             mean_val = param[1]
#             min_val, max_val = param[2], param[3]
#             scale = (max_val - min_val) * 0.01  # Scaling factor based on the parameter range
#             perturbed_val = mean_val + scale * np.random.randn()
#             walker_position.append(perturbed_val)
#         pos0.append(walker_position)
#     return np.array(pos0)



# def chain_init(params_list, ndim, nwalkers):
#     pos0 = []
#     for i in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             min_val, max_val = param[2], param[3]
#             init_val = np.random.uniform(min_val, max_val)
#             walker_position.append(init_val)
#         pos0.append(walker_position + 1e-3 * np.random.randn(ndim))
#     return np.array(pos0)


# def chain_init(params_list, ndim, nwalkers):
#     # with Gaussian dist
#     pos0 = []
#     for _ in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             mean_val = param[1]
#             std_val = np.abs(param[3] - param[2])/6.0  # Assuming 99.7% of values within [min, max]
#             init_val = np.random.normal(mean_val, std_val)
#             walker_position.append(init_val)
#         pos0.append(walker_position)
#     return np.array(pos0)


def chain_init(params_list, ndim, nwalkers):
    pos0 = []
    for _ in range(nwalkers):
        walker_position = []
        for param in params_list:
            min_val, max_val = param[2], param[3]
            init_val = np.random.uniform(min_val, max_val)
            walker_position.append(init_val)
        pos0.append(walker_position)
    return np.array(pos0)


########################################################################################################################
'''

def define_sampler(ndim, 
                   nwalkers, 
                   params_list, 
                   x_grids, 
                   sepia_models, 
                   data, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_labels=None):
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob, args=(params_list, 
                                                                   x_grids, 
                                                                   sepia_models, 
                                                                   data, 
                                                                   fixed_params, 
                                                                   with_underestimation_bias,
                                                                   case_labels))
    return sampler




def do_mcmc(sampler, 
            pos, 
            nrun, 
            ndim,
            if_burn=False
            ):

    time0 = time.time()
    pos, prob, state = sampler.run_mcmc(pos, nrun)

    time1 = time.time()
    print('time (minutes):', (time1 - time0)/60. )

    samples = sampler.chain[:, :, :].reshape((-1, ndim))

    if if_burn: 
        print('Burn-in phase')
        sampler.reset()

    else:
        print('Sampling phase')

    return pos, prob, state, samples, sampler

'''

def mcmc_results(samples):
    results = list(map(lambda v: (v[1], v[2] - v[1], v[1] - v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0))))
    print('mcmc results:', ' '.join(str(result[0]) for result in results))
    return tuple(result[0] for result in results)


#######################################################
## parallel version ######

from multiprocessing import Pool

def define_sampler(ndim, nwalkers, params_list, x_grids, sepia_models, data,
                   fixed_params=None, with_underestimation_bias=False, case_labels=None, pool=None):
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob,
           args=(params_list, x_grids, sepia_models, data, fixed_params,
                 with_underestimation_bias, case_labels), pool=pool)
    return sampler

def do_mcmc(sampler, pos, nrun, ndim, if_burn=False):
    time0 = time.time()
    pos, prob, state = sampler.run_mcmc(pos, nrun)
    print('time (minutes):', (time.time() - time0) / 60.)
    samples = sampler.chain.reshape((-1, ndim))
    if if_burn:
        print('Burn-in phase')
        sampler.reset()
    else:
        print('Sampling phase')
    return pos, prob, state, samples, sampler

