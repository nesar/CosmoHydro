# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_mcmc.ipynb.

# %% auto 0
__all__ = ['ln_prior', 'ln_like', 'ln_prob', 'chain_init', 'define_sampler', 
'do_mcmc', 'mcmc_results', 'log_likelihood']

# %% ../nbs/04_mcmc.ipynb 3

import numpy as np
import emcee 
import time
from cosmo_hydro_emu.emu import emu_redshift, emulate
from cosmo_hydro_emu.load_hacc import PARAM_NAME, delta_cgd


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_label=None):
    """
    Calculate log likelihood for a single model.
    
    With bias parameters (with_underestimation_bias=True):
    - theta contains 5 physical params + 3 bias params: [log_bstar, bCV, bHSE]
    
    Bias application:
    - GSMF: x_eff = x * 10^(log_bstar), y_adjusted = y * bCV
    - fGas: x_eff = x / bHSE
    """
    if fixed_params is None:
        fixed_params = {}

    param_names = PARAM_NAME
    full_params = []

    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1
    
    # Extract bias parameters if using underestimation bias
    # Bias params are at the end of theta: [log_bstar, bCV, bHSE]
    log_bstar = 0.0
    bCV = 1.0
    bHSE = 1.0
    if with_underestimation_bias:
        log_bstar = theta[-3]
        bCV = theta[-2]
        bHSE = theta[-1]
        
        # Check validity
        if bCV <= 0.0 or bHSE <= 0.0:
            return -np.inf
    
    full_params = np.array(full_params)
    
    model_grid, model_var_grid = emulate(sepia_model, full_params)
    
    # Apply delta_cgd correction if specified
    if case_label == 'CGD_nodelta':
        model_grid = model_grid + delta_cgd[:, np.newaxis]
        model_var_grid = model_var_grid + delta_cgd[:, np.newaxis, np.newaxis]

    # Determine effective x based on case and bias
    if with_underestimation_bias:
        if case_label == 'GSMF':
            # Stellar mass bias: shift in log-space -> multiply in linear space
            x_eff = x * (10.0 ** log_bstar)
        elif case_label == 'fGas':
            # Hydrostatic mass bias: divide mass by bHSE
            x_eff = x / bHSE
        else:
            x_eff = x
    else:
        x_eff = x

    # Interpolate model to data points
    model = np.interp(x_eff, x_grid, model_grid[:, 0])
    
    # Adjust y for cosmic variance bias (GSMF only)
    if with_underestimation_bias and case_label == 'GSMF':
        # Cosmic variance bias: multiply number density by bCV
        y_adjusted = y * bCV
    else:
        y_adjusted = y
    
    # Simple chi-squared likelihood (matching 5-param version)
    sigma2 = yerr ** 2
    ll = -0.5 * np.sum((y_adjusted - model) ** 2 / sigma2)
    
    return ll


def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None,
            with_underestimation_bias=False,
            case_labels=None):
    """
    Calculate total log likelihood across all datasets.
    """
    log_likelihoods = []
    case_labels = case_labels.split('_')
    normalize_log_like = False
    
    for i in range(len(sepia_models)):
        ll = log_likelihood(theta, 
                            x_grids[i], 
                            sepia_models[i], 
                            data[i]['x'], 
                            data[i]['y'], 
                            data[i]['yerr'], 
                            fixed_params=fixed_params, 
                            with_underestimation_bias=with_underestimation_bias,
                            case_label=case_labels[i])

        if normalize_log_like: 
            variance = np.var(data[i]['y'])
            log_likelihoods.append(ll / variance)
        else:
            log_likelihoods.append(ll)

    total_ll = sum(log_likelihoods)
    
    return total_ll


def ln_prob(theta, 
            params_list, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):
    """
    Calculate log probability = log prior + log likelihood.
    """
    lp = ln_prior(theta, params_list)
    if not np.isfinite(lp):
        return -np.inf
    return lp + ln_like(theta, x_grids, sepia_models, data, 
                        fixed_params=fixed_params, 
                        with_underestimation_bias=with_underestimation_bias, 
                        case_labels=case_labels)


def ln_prior(theta, params_list, flat_indices=[4, 7]):
    """
    Mixed prior function with option for flat priors on specified parameters.
    
    Parameters:
    -----------
    theta : array-like
        Parameter values being evaluated
    params_list : list
        List of parameter specifications [name, initial_value, lower_bound, upper_bound]
    flat_indices : list or None
        Indices of parameters that should have flat priors. 
        Default is [4] (eps_kin for 5 physical params).
        For 8-param model, you may want [4, 5, 6, 7] to include bias params.
    
    Returns:
    --------
    float
        Log probability of the prior
    """
    if flat_indices is None:
        flat_indices = []  # Default: eps_kin is flat, rest are Gaussian
    
    pdf_sum = 0
    
    for i, (p, param) in enumerate(zip(theta, params_list)):
        # Check if parameter is within bounds
        if not (param[2] < p < param[3]):
            return -np.inf
        
        # Apply flat prior for specified indices, Gaussian prior for others
        if i in flat_indices:
            # Flat prior - contributes nothing to the pdf_sum
            continue
        else:
            # Gaussian prior centered at middle of range
            p_mu = 0.5 * (param[3] - param[2]) + param[2]
            p_sigma = 1 * (param[3] - p_mu)
            pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    
    return pdf_sum

def chain_init(params_list, ndim, nwalkers):
    """
    Initialize walker positions uniformly across parameter ranges.
    """
    pos0 = []
    for _ in range(nwalkers):
        walker_position = []
        for param in params_list:
            min_val, max_val = param[2], param[3]
            init_val = np.random.uniform(min_val, max_val)
            walker_position.append(init_val)
        pos0.append(walker_position)
    return np.array(pos0)


def mcmc_results(samples):
    """
    Calculate median and uncertainties from MCMC samples.
    """
    results = list(map(lambda v: (v[1], v[2] - v[1], v[1] - v[0]), 
                       zip(*np.percentile(samples, [16, 50, 84], axis=0))))
    print('mcmc results:', ' '.join(str(result[0]) for result in results))
    return tuple(result[0] for result in results)


#######################################################
## parallel version ######

from multiprocessing import Pool

def define_sampler(ndim, nwalkers, params_list, x_grids, sepia_models, data,
                   fixed_params=None, with_underestimation_bias=False, 
                   case_labels=None, pool=None):
    """
    Define emcee sampler with optional parallel pool.
    """
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob,
           args=(params_list, x_grids, sepia_models, data, fixed_params,
                 with_underestimation_bias, case_labels), pool=pool)
    return sampler


def do_mcmc(sampler, pos, nrun, ndim, if_burn=False):
    """
    Run MCMC sampling.
    """
    time0 = time.time()
    pos, prob, state = sampler.run_mcmc(pos, nrun)
    print('time (minutes):', (time.time() - time0) / 60.)
    samples = sampler.chain.reshape((-1, ndim))
    if if_burn:
        print('Burn-in phase')
        sampler.reset()
    else:
        print('Sampling phase')
    return pos, prob, state, samples, sampler