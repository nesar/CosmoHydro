# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_mcmc.ipynb.

# %% auto 0
__all__ = ['ln_prior', 'ln_like', 'ln_prob', 'chain_init', 'define_sampler', 
'do_mcmc', 'mcmc_results', 'log_likelihood']

# %% ../nbs/04_mcmc.ipynb 3

import numpy as np
import emcee 
import time
from cosmo_hydro_emu.emu import emu_redshift, emulate
from cosmo_hydro_emu.load_hacc import PARAM_NAME, delta_cgd

'''
def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_label=None):
    if fixed_params is None:
        fixed_params = {}

    param_names = PARAM_NAME
    full_params = []

    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1
    
    if with_underestimation_bias:
        log_f = theta[-1]  # Assume last element is log_f when with_underestimation_bias is True

    
    full_params = np.array(full_params)
    # print(full_params)
    
    model_grid, model_var_grid = emulate(sepia_model, full_params)
    
    if case_label=='CGD': 
        model_grid = model_grid + delta_cgd[:, np.newaxis]
        model_var_grid = model_var_grid + delta_cgd[:, np.newaxis, np.newaxis]

    model = np.interp(x, x_grid, model_grid[:, 0])
    model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
    
    if with_underestimation_bias:
        sigma2 = yerr**2 + model**2 * np.exp(2 * log_f)
        ll = -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))
    else:
        sigma2 = yerr**2
        ll = -0.5 * np.sum(( y - model) ** 2 / sigma2)
        
    return ll


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None, 
                   with_underestimation_bias=False, 
                   case_label=None):
 
    if fixed_params is None: fixed_params={}
    if with_underestimation_bias:
        n_phys=fixed_params.get('n_phys',len(theta)-3)
        phys=theta[:n_phys]; bias=theta[n_phys:]
        log_bstar,bCV,bHSE=bias
    else:
        phys=theta
    t=fixed_params.get('transform',lambda a:a)
    phys_u=t(np.array(phys).reshape(1,-1))[0]
    model_grid, model_var_grid = emulate(sepia_model, phys_u)

    if case_label=='CGD' and fixed_params.get('delta_cgd') is not None:
        d=fixed_params['delta_cgd']
        model_grid=model_grid+d[:,np.newaxis]
        model_var_grid=model_var_grid+d[:,np.newaxis,np.newaxis]
    if with_underestimation_bias:
        if fixed_params.get('ifCalib_OBS',False):
            mod=np.interp(x/bHSE,x_grid,model_grid[:,0])
            mod_var=np.interp(x/bHSE,x_grid,model_var_grid[:,0,0])
        else:
            mod=np.interp(x_grid/bHSE,x_grid,model_grid[:,0])
            mod_var=np.interp(x_grid/bHSE,x_grid,model_var_grid[:,0,0])
        sigma2=yerr**2+mod_var
        ll=-0.5*np.sum((y-mod)**2/sigma2)
    else:
        mod=np.interp(x,x_grid,model_grid[:,0])
        mod_var=np.interp(x,x_grid,model_var_grid[:,0,0])
        sigma2=yerr**2
        ll=-0.5*np.sum((y-mod)**2/sigma2)

    return ll


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None,
                   with_underestimation_bias=False, 
                   case_label=None):

    if fixed_params is None:
        fixed_params = {}
    param_names = PARAM_NAME
    full_params = []
    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1

    if with_underestimation_bias:
        # For example, assume the last three parameters are [log_bstar, bCV, bHSE]
        log_bstar, bCV, bHSE = theta[-3:]
    full_params = np.array(full_params)
    model_grid, model_var_grid = emulate(sepia_model, full_params)
    if case_label=='CGD' and fixed_params.get('delta_cgd') is not None:
        d = fixed_params['delta_cgd']
        model_grid = model_grid + d[:, np.newaxis]
        model_var_grid = model_var_grid + d[:, np.newaxis, np.newaxis]
    if with_underestimation_bias:
        # Use observed x (which has shape (7,)) but possibly rescale it.
        if fixed_params.get('ifCalib_OBS', False):
            # Here we rescale observed x by bHSE so that x_eff still has shape (7,)
            x_eff = x / bHSE
        else:
            x_eff = x
    else:
        x_eff = x
    mod = np.interp(x_eff, x_grid, model_grid[:,0])
    mod_var = np.interp(x_eff, x_grid, model_var_grid[:,0,0])
    if with_underestimation_bias:
        # Here you might combine the observational error with the model variance
        sigma2 = yerr**2 + mod_var  # or use mod**2*np.exp(2*log_f) as needed
        ll = -0.5 * np.sum((y - mod)**2 / sigma2 + np.log(sigma2))
    else:
        sigma2 = yerr**2
        ll = -0.5 * np.sum((y - mod)**2 / sigma2)
    return ll


# def ln_like(theta, x_grids, sepia_models, data, fixed_params=None, with_underestimation_bias=False):
#     log_likelihoods = []
    
#     for i in range(len(sepia_models)):
#         ll = log_likelihood(theta, 
#                             x_grids[i], 
#                             sepia_models[i], 
#                             data[i]['x'], 
#                             data[i]['y'], 
#                             data[i]['yerr'], 
#                             fixed_params=fixed_params, 
#                             with_underestimation_bias=with_underestimation_bias)
#         log_likelihoods.append(ll)

#     total_ll = sum(log_likelihoods)
    
#     return total_ll

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None,
            with_underestimation_bias=False,
            case_labels=None):
    log_likelihoods = []
    case_labels = case_labels.split('_')
    normalize_log_like = True
    
    for i in range(len(sepia_models)):
        ll = log_likelihood(theta, 
                            x_grids[i], 
                            sepia_models[i], 
                            data[i]['x'], 
                            data[i]['y'], 
                            data[i]['yerr'], 
                            fixed_params=fixed_params, 
                            with_underestimation_bias=with_underestimation_bias,
                            case_label=case_labels[i])

        if normalize_log_like: 
            variance = np.var(data[i]['y'])
            log_likelihoods.append(ll / variance)
        else:
            log_likelihoods.append(ll)

    total_ll = sum(log_likelihoods)
    
    return total_ll

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):
    tot=0
    cls=case_labels.split('_') if case_labels else [None]*len(sepia_models)
    norm=fixed_params.get('normalize_log_like',True) if fixed_params else True
    for i in range(len(sepia_models)):
        ll=log_likelihood(theta, x_grids[i], sepia_models[i], data[i]['x'], data[i]['y'], data[i]['yerr'], fixed_params, with_underestimation_bias, cls[i])
        tot += (ll/np.var(data[i]['y'])) if norm else ll
    return tot


def ln_prob(theta, 
            params_list, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):

    lp = ln_prior(theta, params_list)
    if not np.isfinite(lp):
        return -np.inf
    return lp + ln_like(theta, x_grids, sepia_models, data, fixed_params=fixed_params, with_underestimation_bias=with_underestimation_bias, case_labels=case_labels)
'''

'''
def define_sampler(ndim, 
                   nwalkers, 
                   params_list, 
                   x_grids, 
                   sepia_models, 
                   data, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_labels=None):
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob, args=(params_list, 
                                                                   x_grids, 
                                                                   sepia_models, 
                                                                   data, 
                                                                   fixed_params, 
                                                                   with_underestimation_bias,
                                                                   case_labels))
    return sampler

'''




'''
def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None,
                   with_underestimation_bias=False, 
                   case_label=None):
    """
    Calculate log likelihood for a single model.
    
    Parameters:
    -----------
    theta : array-like
        Parameter values
    x_grid : array-like
        Grid of x values for the model
    sepia_model : object
        Trained emulator model
    x, y, yerr : array-like
        Data points and errors
    fixed_params : dict, optional
        Dictionary of fixed parameters
    with_underestimation_bias : bool, optional
        Whether to include bias parameters
    case_label : str, optional
        Label identifying which case this is ('GSMF' or 'fGas')
        
    Returns:
    --------
    ll : float
        Log likelihood value
    """
    if fixed_params is None:
        fixed_params = {}
    
    # Extract physical parameters from theta
    param_names = PARAM_NAME
    full_params = []
    
    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1
    
    full_params = np.array(full_params)
    
    # Apply transformations if specified
    transform = fixed_params.get('transform', lambda a: a)
    full_params_transformed = transform(np.array(full_params).reshape(1, -1))[0]
    
    # Get model predictions
    model_grid, model_var_grid = emulate(sepia_model, full_params_transformed)
    
    # Apply delta_cgd if specified and if case is 'CGD'
    if case_label == 'CGD' and fixed_params.get('delta_cgd') is not None:
        d = fixed_params['delta_cgd']
        model_grid = model_grid + d[:, np.newaxis]
        model_var_grid = model_var_grid + d[:, np.newaxis, np.newaxis]
    
    # Handle bias parameters
    if with_underestimation_bias:
        # Extract bias parameters based on case
        if case_label == 'GSMF':
            # For stellar mass function, apply stellar mass bias and cosmic variance
            log_bstar = theta[-3]  # Third to last parameter
            bCV = theta[-2]        # Second to last parameter
            
            # Apply biases to x or y depending on if calibrating against observations
            if fixed_params.get('ifCalib_OBS', False):
                # Scale x by stellar mass bias
                x_eff = x * (10**log_bstar)
                model = np.interp(x_eff, x_grid, model_grid[:, 0])
                model_var = np.interp(x_eff, x_grid, model_var_grid[:, 0, 0])
                # Add cosmic variance bias to y
                y_adjusted = y + np.log10(bCV)
            else:
                # For emulator testing, apply biases to grid instead
                x_eff = x_grid * (10**log_bstar)
                model = np.interp(x_eff, x_grid, model_grid[:, 0])
                model_var = np.interp(x_eff, x_grid, model_var_grid[:, 0, 0])
                y_adjusted = y + np.log10(bCV)
            
            # Calculate log likelihood
            sigma2 = yerr**2 + model_var
            ll = -0.5 * np.sum((y_adjusted - model)**2 / sigma2)
            
        elif case_label == 'fGas':
            # For gas fraction, apply hydrostatic mass bias
            bHSE = theta[-1]  # Last parameter
            
            # Apply HSE bias to x
            if fixed_params.get('ifCalib_OBS', False):
                x_eff = x / bHSE
                model = np.interp(x_eff, x_grid, model_grid[:, 0])
                model_var = np.interp(x_eff, x_grid, model_var_grid[:, 0, 0])
            else:
                x_eff = x_grid / bHSE
                model = np.interp(x_eff, x_grid, model_grid[:, 0])
                model_var = np.interp(x_eff, x_grid, model_var_grid[:, 0, 0])
            
            # Calculate log likelihood
            sigma2 = yerr**2 + model_var
            ll = -0.5 * np.sum((y - model)**2 / sigma2)
        
        else:
            # For other cases, don't apply bias
            model = np.interp(x, x_grid, model_grid[:, 0])
            model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
            sigma2 = yerr**2
            ll = -0.5 * np.sum((y - model)**2 / sigma2)
    
    else:
        # No bias parameters
        model = np.interp(x, x_grid, model_grid[:, 0])
        model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
        sigma2 = yerr**2
        ll = -0.5 * np.sum((y - model)**2 / sigma2)
    
    return ll


'''


'''
def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None,
                   with_underestimation_bias=False, 
                   case_label=None):
    """
    Calculate log likelihood for a single model.
    """
    if fixed_params is None:
        fixed_params = {}
    
    # Extract physical parameters from theta
    param_names = PARAM_NAME
    full_params = []
    
    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1
    
    full_params = np.array(full_params)
    
    # Apply transformations if specified
    transform = fixed_params.get('transform', lambda a: a)
    full_params_transformed = transform(np.array(full_params).reshape(1, -1))[0]
    
    # Get model predictions
    model_grid, model_var_grid = emulate(sepia_model, full_params_transformed)
    
    # Apply delta_cgd if specified and if case is 'CGD'
    if case_label == 'CGD' and fixed_params.get('delta_cgd') is not None:
        d = fixed_params['delta_cgd']
        model_grid = model_grid + d[:, np.newaxis]
        model_var_grid = model_var_grid + d[:, np.newaxis, np.newaxis]
    
    # Handle bias parameters
    if with_underestimation_bias:
        # Extract bias parameters based on case
        if case_label == 'GSMF':
            # For stellar mass function, apply stellar mass bias and cosmic variance
            log_bstar = theta[-3]  # Third to last parameter
            bCV = theta[-2]        # Second to last parameter
            
            # Apply biases to x or y depending on if calibrating against observations
            if fixed_params.get('ifCalib_OBS', False):
                # Scale x by stellar mass bias
                x_eff = x * (10**log_bstar)
                
                # Make sure we interpolate to get values at exactly the data points
                model = np.interp(x_eff, x_grid, model_grid[:, 0])
                model_var = np.interp(x_eff, x_grid, model_var_grid[:, 0, 0])
                
                # Add cosmic variance bias to y
                y_adjusted = y + np.log10(bCV)
            else:
                # For emulator testing, apply biases to grid instead
                x_grid_biased = x_grid * (10**log_bstar)
                
                # Make sure we interpolate at the data points
                model = np.interp(x, x_grid_biased, model_grid[:, 0])
                model_var = np.interp(x, x_grid_biased, model_var_grid[:, 0, 0])
                
                y_adjusted = y + np.log10(bCV)
            
            # Ensure shapes match
            if model.shape != y_adjusted.shape or model_var.shape != yerr.shape:
                print(f"Shape mismatch in GSMF case - model: {model.shape}, y: {y_adjusted.shape}, "
                      f"model_var: {model_var.shape}, yerr: {yerr.shape}")
                
                # Adjust shapes by interpolating to the data points if needed
                if model.shape != y_adjusted.shape:
                    if len(x_eff) != len(y_adjusted):
                        x_common = x  # Use original x values
                        model = np.interp(x_common, x_eff if len(x_eff) > 1 else [x_eff[0], x_eff[0]+1], 
                                         model if len(model) > 1 else [model[0], model[0]])
                
                if model_var.shape != yerr.shape:
                    if len(x_eff) != len(yerr):
                        x_common = x  # Use original x values
                        model_var = np.interp(x_common, x_eff if len(x_eff) > 1 else [x_eff[0], x_eff[0]+1], 
                                             model_var if len(model_var) > 1 else [model_var[0], model_var[0]])
            
            # Calculate log likelihood
            sigma2 = yerr**2 + model_var
            ll = -0.5 * np.sum((y_adjusted - model)**2 / sigma2)
            
        elif case_label == 'fGas':
            # For gas fraction, apply hydrostatic mass bias
            bHSE = theta[-1]  # Last parameter
            
            # Apply HSE bias to x
            if fixed_params.get('ifCalib_OBS', False):
                x_eff = x / bHSE
                model = np.interp(x_eff, x_grid, model_grid[:, 0])
                model_var = np.interp(x_eff, x_grid, model_var_grid[:, 0, 0])
            else:
                x_grid_biased = x_grid / bHSE
                model = np.interp(x, x_grid_biased, model_grid[:, 0])
                model_var = np.interp(x, x_grid_biased, model_var_grid[:, 0, 0])
            
            # Ensure shapes match
            if model.shape != y.shape or model_var.shape != yerr.shape:
                print(f"Shape mismatch in fGas case - model: {model.shape}, y: {y.shape}, "
                      f"model_var: {model_var.shape}, yerr: {yerr.shape}")
                
                # Adjust shapes by interpolating to the data points if needed
                if model.shape != y.shape:
                    if len(x_eff) != len(y):
                        x_common = x  # Use original x values
                        model = np.interp(x_common, x_eff if len(x_eff) > 1 else [x_eff[0], x_eff[0]+1], 
                                         model if len(model) > 1 else [model[0], model[0]])
                
                if model_var.shape != yerr.shape:
                    if len(x_eff) != len(yerr):
                        x_common = x  # Use original x values
                        model_var = np.interp(x_common, x_eff if len(x_eff) > 1 else [x_eff[0], x_eff[0]+1], 
                                             model_var if len(model_var) > 1 else [model_var[0], model_var[0]])
            
            # Calculate log likelihood
            sigma2 = yerr**2 + model_var
            ll = -0.5 * np.sum((y - model)**2 / sigma2)
        
        else:
            # For other cases, don't apply bias
            model = np.interp(x, x_grid, model_grid[:, 0])
            model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
            sigma2 = yerr**2 + model_var
            ll = -0.5 * np.sum((y - model)**2 / sigma2)
    
    else:
        # No bias parameters
        model = np.interp(x, x_grid, model_grid[:, 0])
        model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
        sigma2 = yerr**2 + model_var
        ll = -0.5 * np.sum((y - model)**2 / sigma2)
    
    return ll

    

'''

def log_likelihood(theta, x_grid, sepia_model, x, y, yerr,
                   fixed_params=None, with_underestimation_bias=False, case_label=None):
    if fixed_params is None: fixed_params = {}
    th = np.asarray(theta, float)

    full=[]; ti=0
    for p in PARAM_NAME:
        if p in fixed_params: full.append(fixed_params[p])
        else:
            if ti >= len(th): return -np.inf
            full.append(th[ti]); ti += 1
    full = np.asarray(full, float)

    log_bstar=0.0; bCV=1.0; bHSE=1.0
    if with_underestimation_bias:
        if len(th) < ti+3:
            strict = bool(fixed_params.get("strict_bias", False))
            msg = f"with_underestimation_bias=True but theta has {len(th)} elems; need >= {ti+3}. Using (0,1,1)."
            if strict: raise ValueError(msg)
            if not getattr(log_likelihood, "_warned_biaslen", False):
                print("WARNING:", msg)
                log_likelihood._warned_biaslen = True
        else:
            log_bstar=float(th[ti+0]); bCV=float(th[ti+1]); bHSE=float(th[ti+2])

    if (bCV <= 0.0) or (bHSE <= 0.0): return -np.inf

    transform = fixed_params.get('transform', lambda a: a)
    full_u = transform(full.reshape(1,-1))[0]

    mgrid, vgrid = emulate(sepia_model, full_u)
    mgrid = np.asarray(mgrid)
    mgrid = (mgrid[:,0] if (mgrid.ndim==2 and mgrid.shape[1]==1) else mgrid.squeeze())

    vgrid = np.asarray(vgrid)
    vgrid = (vgrid[:,0,0] if vgrid.ndim==3 else vgrid.squeeze())
    vgrid = np.maximum(vgrid, 0.0)

    x = np.asarray(x, float); xg = np.asarray(x_grid, float)

    if with_underestimation_bias and case_label == 'GSMF':
        x_eff = x * (10.0**log_bstar)
    elif with_underestimation_bias and case_label == 'fGas':
        x_eff = x / bHSE
    else:
        x_eff = x

    mod = np.interp(x_eff, xg, mgrid)
    modv = np.interp(x_eff, xg, vgrid)
    modv = np.maximum(modv, 0.0)

    y = np.asarray(y, float)
    yerr = np.asarray(yerr, float)
    sigma = (0.5*(yerr[0]+yerr[1]) if (yerr.ndim==2 and yerr.shape[0]==2) else yerr)

    if case_label == 'GSMF':
        eps=1e-300
        y_lin = np.maximum(y, eps)
        m_lin = np.maximum(mod, eps)
        y_log = np.log10(y_lin) + (np.log10(bCV) if with_underestimation_bias else 0.0)
        m_log = np.log10(m_lin)
        sig_d = sigma/(y_lin*np.log(10.0))
        sig_m = np.sqrt(modv)/(m_lin*np.log(10.0))
        s2 = sig_d**2 + sig_m**2
        return -0.5*np.sum((y_log-m_log)**2/s2 + np.log(s2))
    else:
        s2 = sigma**2 + modv
        return -0.5*np.sum((y-mod)**2/s2 + np.log(s2))


'''

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):
    """
    Calculate the total log likelihood for multiple datasets.
    
    Parameters:
    -----------
    theta : array-like
        Parameter values
    x_grids : list
        List of x grids for each model
    sepia_models : list
        List of trained emulator models
    data : list of dict
        List of data dictionaries, each containing 'x', 'y', 'yerr'
    fixed_params : dict, optional
        Dictionary of fixed parameters
    with_underestimation_bias : bool, optional
        Whether to include bias parameters
    case_labels : str, optional
        String of case labels separated by underscore
        
    Returns:
    --------
    tot : float
        Total log likelihood
    """
    tot = 0
    
    # Parse case labels
    if case_labels:
        cls = case_labels.split('_')
    else:
        cls = [None] * len(sepia_models)
    
    # Get normalization flag
    normalize = fixed_params.get('normalize_log_like', True) if fixed_params else True
    
    # Calculate log likelihood for each dataset
    for i in range(len(sepia_models)):
        ll = log_likelihood(
            theta, 
            x_grids[i], 
            sepia_models[i], 
            data[i]['x'], 
            data[i]['y'], 
            data[i]['yerr'], 
            fixed_params, 
            with_underestimation_bias, 
            cls[i]
        )
        
        # Normalize by variance if requested
        if normalize:
            tot += ll / np.var(data[i]['y'])
        else:
            tot += ll
            
    return tot

def ln_prob(theta, 
            params_list, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):
    """
    Calculate the log probability.
    
    Parameters:
    -----------
    (see ln_like for parameter descriptions)
    
    Returns:
    --------
    float : Log probability or -infinity if outside prior
    """
    # Calculate log prior
    lp = ln_prior(theta, params_list)
    
    # Return -infinity if outside prior bounds
    if not np.isfinite(lp):
        return -np.inf
    
    # Return log prior + log likelihood
    return lp + ln_like(
        theta, 
        x_grids, 
        sepia_models, 
        data, 
        fixed_params=fixed_params, 
        with_underestimation_bias=with_underestimation_bias, 
        case_labels=case_labels
    )


'''

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None,
            with_underestimation_bias=False,
            case_labels=None):
    log_likelihoods = []
    case_labels = case_labels.split('_')
    normalize_log_like = False
    
    for i in range(len(sepia_models)):
        ll = log_likelihood(theta, 
                            x_grids[i], 
                            sepia_models[i], 
                            data[i]['x'], 
                            data[i]['y'], 
                            data[i]['yerr'], 
                            fixed_params=fixed_params, 
                            with_underestimation_bias=with_underestimation_bias,
                            case_label=case_labels[i])

        if normalize_log_like: 
            variance = np.var(data[i]['y'])
            log_likelihoods.append(ll / variance)
        else:
            log_likelihoods.append(ll)

    total_ll = sum(log_likelihoods)
    
    return total_ll


def ln_prob(theta, 
            params_list, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):

    lp = ln_prior(theta, params_list)
    if not np.isfinite(lp):
        return -np.inf
    return lp + ln_like(theta, x_grids, sepia_models, data, fixed_params=fixed_params, with_underestimation_bias=with_underestimation_bias, case_labels=case_labels)



## Default prior 

'''
def ln_prior(theta, params_list):
    pdf_sum = 0
    for p, param in zip(theta, params_list):
        if not (param[2] < p < param[3]):
            return -np.inf
        p_mu = 0.5 * (param[3] - param[2]) + param[2]
        p_sigma = 1 * (param[3] - p_mu)
        pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    return pdf_sum

def ln_prior(theta, params_list):
    s=0
    for p,par in zip(theta,params_list):
        lo,hi=par[2],par[3]
        if not(lo<p<hi): return -np.inf
        mu=0.5*(lo+hi); sigma=hi-mu
        s+=np.log(1/(np.sqrt(2*np.pi)*sigma))-0.5*((p-mu)/sigma)**2
    return s


'''

# Default prior

def ln_prior(theta, params_list):
    pdf_sum = 0
    for p, param in zip(theta, params_list):
        if not (param[2] < p < param[3]):
            return -np.inf
        p_mu = 0.5 * (param[3] - param[2]) + param[2]
        p_sigma = 1 * (param[3] - p_mu)
        pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    return pdf_sum


# def ln_prior(theta, params_list):
    
#     """
#     Mixed prior function that allows specification of which parameters should have flat priors.
    
#     Parameters:
#     -----------
#     theta : array-like
#         Parameter values being evaluated
#     params_list : list
#         List of parameter specifications [name, initial_value, lower_bound, upper_bound]
#     flat_indices : list or None
#         Indices of parameters that should have flat priors. If None, all priors are Gaussian.
    
#     Returns:
#     --------
#     float
#         Log probability of the prior
#     """

#     flat_indices=[4] # 4 == eps_kin is flat, rest are gaussian

#     if flat_indices is None:
#         flat_indices = []
    
#     pdf_sum = 0
    
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         # Check if parameter is within bounds
#         if not (param[2] < p < param[3]):
#             return -np.inf
        
#         # Apply flat prior for specified indices, Gaussian prior for others
#         if i in flat_indices:
#             # Flat prior - contributes nothing to the pdf_sum
#             continue
#         else:
#             # Gaussian prior - same as your original function
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)
#             pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    
#     return pdf_sum
# ########################################################################################################################


########################################################################################################################


### Strong prior for EPS > 5
# from scipy.stats import halfnorm

# def ln_prior(theta, params_list):
#     pdf_sum = 0
#     p_loc = 0.55
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         if not (param[2] < p < param[3]):
#             return -np.inf
        
#         if i == len(theta) - 1:  # Last parameter
#             if p < p_loc:
#                 return -np.inf
#             p_sigma = 0.5 * (param[3] - p_loc)
#             pdf = halfnorm(loc=p_loc, scale=p_sigma).pdf(p)
#         else:
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)
#             pdf = (1.0 / (np.sqrt(2 * np.pi) * p_sigma)) * np.exp(-0.5 * (p - p_mu) ** 2 / p_sigma ** 2)
        
#         pdf_sum += np.log(pdf)
    
#     return pdf_sum


### Very Strong prior for 3 parameters

########################################################################################################################

# def ln_prior(theta, params_list):

#     prior_info_mask = [True, True, True, False, False]
#     prior_locs = [3.0, 0.5, 0.8]

#     pdf_sum = 0
#     prior_index = 0
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         if not (param[2] < p < param[3]):
#             return -np.inf

#         if prior_info_mask[i]:
#             p_mu = prior_locs[prior_index]
#             p_sigma = 0.05 * (param[3] - param[2])  # Tight prior
#             prior_index += 1
#         else:
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)  # Uninformed prior
        
#         pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
        
#     return pdf_sum


############################################################
########################################################################################################################

def chain_init(params_list, ndim, nwalkers):
    pos0 = [[param[1] * 1.0 for param in params_list] + 1e-3 * np.random.randn(ndim) for _ in range(nwalkers)]
    return pos0


# def chain_init(params_list, ndim, nwalkers):
#     pos0 = []
#     for _ in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             mean_val = param[1]
#             min_val, max_val = param[2], param[3]
#             scale = (max_val - min_val) * 0.01  # Scaling factor based on the parameter range
#             perturbed_val = mean_val + scale * np.random.randn()
#             walker_position.append(perturbed_val)
#         pos0.append(walker_position)
#     return np.array(pos0)



# def chain_init(params_list, ndim, nwalkers):
#     pos0 = []
#     for i in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             min_val, max_val = param[2], param[3]
#             init_val = np.random.uniform(min_val, max_val)
#             walker_position.append(init_val)
#         pos0.append(walker_position + 1e-3 * np.random.randn(ndim))
#     return np.array(pos0)


# def chain_init(params_list, ndim, nwalkers):
#     # with Gaussian dist
#     pos0 = []
#     for _ in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             mean_val = param[1]
#             std_val = np.abs(param[3] - param[2])/6.0  # Assuming 99.7% of values within [min, max]
#             init_val = np.random.normal(mean_val, std_val)
#             walker_position.append(init_val)
#         pos0.append(walker_position)
#     return np.array(pos0)


def chain_init(params_list, ndim, nwalkers):
    pos0 = []
    for _ in range(nwalkers):
        walker_position = []
        for param in params_list:
            min_val, max_val = param[2], param[3]
            init_val = np.random.uniform(min_val, max_val)
            walker_position.append(init_val)
        pos0.append(walker_position)
    return np.array(pos0)


########################################################################################################################

def mcmc_results(samples):
    results = list(map(lambda v: (v[1], v[2] - v[1], v[1] - v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0))))
    print('mcmc results:', ' '.join(str(result[0]) for result in results))
    return tuple(result[0] for result in results)


#######################################################
## parallel version ######

from multiprocessing import Pool

def define_sampler(ndim, nwalkers, params_list, x_grids, sepia_models, data,
                   fixed_params=None, with_underestimation_bias=False, case_labels=None, pool=None):
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob,
           args=(params_list, x_grids, sepia_models, data, fixed_params,
                 with_underestimation_bias, case_labels), pool=pool)
    return sampler

def do_mcmc(sampler, pos, nrun, ndim, if_burn=False):
    time0 = time.time()
    pos, prob, state = sampler.run_mcmc(pos, nrun)
    print('time (minutes):', (time.time() - time0) / 60.)
    samples = sampler.chain.reshape((-1, ndim))
    if if_burn:
        print('Burn-in phase')
        sampler.reset()
    else:
        print('Sampling phase')
    return pos, prob, state, samples, sampler