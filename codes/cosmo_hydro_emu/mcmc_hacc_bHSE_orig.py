# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_mcmc.ipynb.

# %% auto 0
__all__ = ['ln_prior', 'ln_like', 'ln_prob', 'chain_init', 'define_sampler', 
'do_mcmc', 'mcmc_results', 'log_likelihood']

# %% ../nbs/04_mcmc.ipynb 3

import numpy as np
import emcee 
import time
from cosmo_hydro_emu.emu import emu_redshift, emulate
from cosmo_hydro_emu.load_hacc import PARAM_NAME, delta_cgd


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_label=None):
    if fixed_params is None:
        fixed_params = {}

    param_names = PARAM_NAME
    full_params = []

    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1
    
    if with_underestimation_bias:
        log_f = theta[-1]  # Assume last element is log_f when with_underestimation_bias is True

    
    full_params = np.array(full_params)
    # print(full_params)
    
    model_grid, model_var_grid = emulate(sepia_model, full_params)
    
    if case_label=='CGD': 
        model_grid = model_grid + delta_cgd[:, np.newaxis]
        model_var_grid = model_var_grid + delta_cgd[:, np.newaxis, np.newaxis]

    model = np.interp(x, x_grid, model_grid[:, 0])
    model_var = np.interp(x, x_grid, model_var_grid[:, 0, 0])
    
    if with_underestimation_bias:
        sigma2 = yerr**2 + model**2 * np.exp(2 * log_f)
        ll = -0.5 * np.sum((y - model) ** 2 / sigma2 + np.log(sigma2))
    else:
        sigma2 = yerr**2
        ll = -0.5 * np.sum(( y - model) ** 2 / sigma2)
        
    return ll


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None, 
                   with_underestimation_bias=False, 
                   case_label=None):
 
    if fixed_params is None: fixed_params={}
    if with_underestimation_bias:
        n_phys=fixed_params.get('n_phys',len(theta)-3)
        phys=theta[:n_phys]; bias=theta[n_phys:]
        log_bstar,bCV,bHSE=bias
    else:
        phys=theta
    t=fixed_params.get('transform',lambda a:a)
    phys_u=t(np.array(phys).reshape(1,-1))[0]
    model_grid, model_var_grid = emulate(sepia_model, phys_u)

    if case_label=='CGD' and fixed_params.get('delta_cgd') is not None:
        d=fixed_params['delta_cgd']
        model_grid=model_grid+d[:,np.newaxis]
        model_var_grid=model_var_grid+d[:,np.newaxis,np.newaxis]
    if with_underestimation_bias:
        if fixed_params.get('ifCalib_OBS',False):
            mod=np.interp(x/bHSE,x_grid,model_grid[:,0])
            mod_var=np.interp(x/bHSE,x_grid,model_var_grid[:,0,0])
        else:
            mod=np.interp(x_grid/bHSE,x_grid,model_grid[:,0])
            mod_var=np.interp(x_grid/bHSE,x_grid,model_var_grid[:,0,0])
        sigma2=yerr**2+mod_var
        ll=-0.5*np.sum((y-mod)**2/sigma2)
    else:
        mod=np.interp(x,x_grid,model_grid[:,0])
        mod_var=np.interp(x,x_grid,model_var_grid[:,0,0])
        sigma2=yerr**2
        ll=-0.5*np.sum((y-mod)**2/sigma2)

    return ll


def log_likelihood(theta, 
                   x_grid, 
                   sepia_model, 
                   x, y, yerr, 
                   fixed_params=None,
                   with_underestimation_bias=False, 
                   case_label=None):

    if fixed_params is None:
        fixed_params = {}
    param_names = PARAM_NAME
    full_params = []
    theta_index = 0
    for param in param_names:
        if param in fixed_params:
            full_params.append(fixed_params[param])
        else:
            full_params.append(theta[theta_index])
            theta_index += 1

    if with_underestimation_bias:
        # For example, assume the last three parameters are [log_bstar, bCV, bHSE]
        log_bstar, bCV, bHSE = theta[-3:]
    full_params = np.array(full_params)
    model_grid, model_var_grid = emulate(sepia_model, full_params)
    if case_label=='CGD' and fixed_params.get('delta_cgd') is not None:
        d = fixed_params['delta_cgd']
        model_grid = model_grid + d[:, np.newaxis]
        model_var_grid = model_var_grid + d[:, np.newaxis, np.newaxis]
    if with_underestimation_bias:
        # Use observed x (which has shape (7,)) but possibly rescale it.
        if fixed_params.get('ifCalib_OBS', False):
            # Here we rescale observed x by bHSE so that x_eff still has shape (7,)
            x_eff = x / bHSE
        else:
            x_eff = x
    else:
        x_eff = x
    mod = np.interp(x_eff, x_grid, model_grid[:,0])
    mod_var = np.interp(x_eff, x_grid, model_var_grid[:,0,0])
    if with_underestimation_bias:
        # Here you might combine the observational error with the model variance
        sigma2 = yerr**2 + mod_var  # or use mod**2*np.exp(2*log_f) as needed
        ll = -0.5 * np.sum((y - mod)**2 / sigma2 + np.log(sigma2))
    else:
        sigma2 = yerr**2
        ll = -0.5 * np.sum((y - mod)**2 / sigma2)
    return ll


# def ln_like(theta, x_grids, sepia_models, data, fixed_params=None, with_underestimation_bias=False):
#     log_likelihoods = []
    
#     for i in range(len(sepia_models)):
#         ll = log_likelihood(theta, 
#                             x_grids[i], 
#                             sepia_models[i], 
#                             data[i]['x'], 
#                             data[i]['y'], 
#                             data[i]['yerr'], 
#                             fixed_params=fixed_params, 
#                             with_underestimation_bias=with_underestimation_bias)
#         log_likelihoods.append(ll)

#     total_ll = sum(log_likelihoods)
    
#     return total_ll

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None,
            with_underestimation_bias=False,
            case_labels=None):
    log_likelihoods = []
    case_labels = case_labels.split('_')
    normalize_log_like = True
    
    for i in range(len(sepia_models)):
        ll = log_likelihood(theta, 
                            x_grids[i], 
                            sepia_models[i], 
                            data[i]['x'], 
                            data[i]['y'], 
                            data[i]['yerr'], 
                            fixed_params=fixed_params, 
                            with_underestimation_bias=with_underestimation_bias,
                            case_label=case_labels[i])

        if normalize_log_like: 
            variance = np.var(data[i]['y'])
            log_likelihoods.append(ll / variance)
        else:
            log_likelihoods.append(ll)

    total_ll = sum(log_likelihoods)
    
    return total_ll

def ln_like(theta, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):
    tot=0
    cls=case_labels.split('_') if case_labels else [None]*len(sepia_models)
    norm=fixed_params.get('normalize_log_like',True) if fixed_params else True
    for i in range(len(sepia_models)):
        ll=log_likelihood(theta, x_grids[i], sepia_models[i], data[i]['x'], data[i]['y'], data[i]['yerr'], fixed_params, with_underestimation_bias, cls[i])
        tot += (ll/np.var(data[i]['y'])) if norm else ll
    return tot


def ln_prob(theta, 
            params_list, 
            x_grids, 
            sepia_models, 
            data, 
            fixed_params=None, 
            with_underestimation_bias=False, 
            case_labels=None):

    lp = ln_prior(theta, params_list)
    if not np.isfinite(lp):
        return -np.inf
    return lp + ln_like(theta, x_grids, sepia_models, data, fixed_params=fixed_params, with_underestimation_bias=with_underestimation_bias, case_labels=case_labels)


'''
def define_sampler(ndim, 
                   nwalkers, 
                   params_list, 
                   x_grids, 
                   sepia_models, 
                   data, 
                   fixed_params=None, 
                   with_underestimation_bias=False,
                   case_labels=None):
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob, args=(params_list, 
                                                                   x_grids, 
                                                                   sepia_models, 
                                                                   data, 
                                                                   fixed_params, 
                                                                   with_underestimation_bias,
                                                                   case_labels))
    return sampler

'''




## Default prior 

def ln_prior(theta, params_list):
    pdf_sum = 0
    for p, param in zip(theta, params_list):
        if not (param[2] < p < param[3]):
            return -np.inf
        p_mu = 0.5 * (param[3] - param[2]) + param[2]
        p_sigma = 1 * (param[3] - p_mu)
        pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
    return pdf_sum

def ln_prior(theta, params_list):
    s=0
    for p,par in zip(theta,params_list):
        lo,hi=par[2],par[3]
        if not(lo<p<hi): return -np.inf
        mu=0.5*(lo+hi); sigma=hi-mu
        s+=np.log(1/(np.sqrt(2*np.pi)*sigma))-0.5*((p-mu)/sigma)**2
    return s


########################################################################################################################


### Strong prior for EPS > 5
# from scipy.stats import halfnorm

# def ln_prior(theta, params_list):
#     pdf_sum = 0
#     p_loc = 0.55
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         if not (param[2] < p < param[3]):
#             return -np.inf
        
#         if i == len(theta) - 1:  # Last parameter
#             if p < p_loc:
#                 return -np.inf
#             p_sigma = 0.5 * (param[3] - p_loc)
#             pdf = halfnorm(loc=p_loc, scale=p_sigma).pdf(p)
#         else:
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)
#             pdf = (1.0 / (np.sqrt(2 * np.pi) * p_sigma)) * np.exp(-0.5 * (p - p_mu) ** 2 / p_sigma ** 2)
        
#         pdf_sum += np.log(pdf)
    
#     return pdf_sum


### Very Strong prior for 3 parameters

########################################################################################################################

# def ln_prior(theta, params_list):

#     prior_info_mask = [True, True, True, False, False]
#     prior_locs = [3.0, 0.5, 0.8]

#     pdf_sum = 0
#     prior_index = 0
#     for i, (p, param) in enumerate(zip(theta, params_list)):
#         if not (param[2] < p < param[3]):
#             return -np.inf

#         if prior_info_mask[i]:
#             p_mu = prior_locs[prior_index]
#             p_sigma = 0.05 * (param[3] - param[2])  # Tight prior
#             prior_index += 1
#         else:
#             p_mu = 0.5 * (param[3] - param[2]) + param[2]
#             p_sigma = 1 * (param[3] - p_mu)  # Uninformed prior
        
#         pdf_sum += np.log(1.0 / (np.sqrt(2 * np.pi) * p_sigma)) - 0.5 * (p - p_mu) ** 2 / p_sigma ** 2
        
#     return pdf_sum


############################################################
########################################################################################################################

def chain_init(params_list, ndim, nwalkers):
    pos0 = [[param[1] * 1.0 for param in params_list] + 1e-3 * np.random.randn(ndim) for _ in range(nwalkers)]
    return pos0


# def chain_init(params_list, ndim, nwalkers):
#     pos0 = []
#     for _ in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             mean_val = param[1]
#             min_val, max_val = param[2], param[3]
#             scale = (max_val - min_val) * 0.01  # Scaling factor based on the parameter range
#             perturbed_val = mean_val + scale * np.random.randn()
#             walker_position.append(perturbed_val)
#         pos0.append(walker_position)
#     return np.array(pos0)



# def chain_init(params_list, ndim, nwalkers):
#     pos0 = []
#     for i in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             min_val, max_val = param[2], param[3]
#             init_val = np.random.uniform(min_val, max_val)
#             walker_position.append(init_val)
#         pos0.append(walker_position + 1e-3 * np.random.randn(ndim))
#     return np.array(pos0)


# def chain_init(params_list, ndim, nwalkers):
#     # with Gaussian dist
#     pos0 = []
#     for _ in range(nwalkers):
#         walker_position = []
#         for param in params_list:
#             mean_val = param[1]
#             std_val = np.abs(param[3] - param[2])/6.0  # Assuming 99.7% of values within [min, max]
#             init_val = np.random.normal(mean_val, std_val)
#             walker_position.append(init_val)
#         pos0.append(walker_position)
#     return np.array(pos0)


def chain_init(params_list, ndim, nwalkers):
    pos0 = []
    for _ in range(nwalkers):
        walker_position = []
        for param in params_list:
            min_val, max_val = param[2], param[3]
            init_val = np.random.uniform(min_val, max_val)
            walker_position.append(init_val)
        pos0.append(walker_position)
    return np.array(pos0)


########################################################################################################################

'''

def do_mcmc(sampler, 
            pos, 
            nrun, 
            ndim,
            if_burn=False
            ):

    time0 = time.time()
    pos, prob, state = sampler.run_mcmc(pos, nrun)

    time1 = time.time()
    print('time (minutes):', (time1 - time0)/60. )

    samples = sampler.chain[:, :, :].reshape((-1, ndim))

    if if_burn: 
        print('Burn-in phase')
        sampler.reset()

    else:
        print('Sampling phase')

    return pos, prob, state, samples, sampler
'''


def mcmc_results(samples):
    results = list(map(lambda v: (v[1], v[2] - v[1], v[1] - v[0]), zip(*np.percentile(samples, [16, 50, 84], axis=0))))
    print('mcmc results:', ' '.join(str(result[0]) for result in results))
    return tuple(result[0] for result in results)


from multiprocessing import Pool

def define_sampler(ndim, nwalkers, params_list, x_grids, sepia_models, data,
                   fixed_params=None, with_underestimation_bias=False, case_labels=None, pool=None):
    sampler = emcee.EnsembleSampler(nwalkers, ndim, ln_prob,
           args=(params_list, x_grids, sepia_models, data, fixed_params,
                 with_underestimation_bias, case_labels), pool=pool)
    return sampler

def do_mcmc(sampler, pos, nrun, ndim, if_burn=False):
    time0 = time.time()
    pos, prob, state = sampler.run_mcmc(pos, nrun)
    print('time (minutes):', (time.time() - time0) / 60.)
    samples = sampler.chain.reshape((-1, ndim))
    if if_burn:
        print('Burn-in phase')
        sampler.reset()
    else:
        print('Sampling phase')
    return pos, prob, state, samples, sampler

